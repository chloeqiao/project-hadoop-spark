{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute tf-idf score using MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wget https://www.dropbox.com/s/9t4my8o4g3fibwk/tmapper.py\n",
    "wget https://www.dropbox.com/s/kst1uc3so0sugrz/treducer.py\n",
    "wget https://www.dropbox.com/s/rojbnquhqsd3oa7/tfmapper.py\n",
    "wget https://www.dropbox.com/s/xat1ew4l3pxpokr/tfreducer.py\n",
    "wget https://www.dropbox.com/s/13p3c6hvroef2b9/dmapper.py\n",
    "wget https://www.dropbox.com/s/s775nolj6ltdq03/dreducer.py\n",
    "wget https://www.dropbox.com/s/8ahm8fa9m8r0ksw/tfidfmapper.py\n",
    "chmod +x *.py\n",
    "    \n",
    "\n",
    "# compute term occurance\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/hadoop/proj/input \\\n",
    "-output /user/hadoop/proj/output \\\n",
    "-file /home/hadoop/tmapper.py \\\n",
    "-mapper /home/hadoop/tmapper.py \\\n",
    "-file /home/hadoop/treducer.py \\\n",
    "-reducer /home/hadoop/treducer.py\n",
    "\n",
    "#hdfs dfs -ls /user/hadoop/proj/output/\n",
    "hdfs dfs -rm /user/hadoop/proj/output/_SUCCESS\n",
    "\n",
    "#hdfs dfs -cat /user/hadoop/proj/output/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute number of words in each file and combine with the previous output\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D stream.non.zero.exit.is.failure=false \\\n",
    "-input /user/hadoop/proj/output \\\n",
    "-output /user/hadoop/proj/output1 \\\n",
    "-file /home/hadoop/tfmapper.py \\\n",
    "-mapper /home/hadoop/tfmapper.py \\\n",
    "-file /home/hadoop/tfreducer.py \\\n",
    "-reducer /home/hadoop/tfreducer.py\n",
    "\n",
    "#hdfs dfs -ls /user/hadoop/proj/output1/\n",
    "hdfs dfs -rm /user/hadoop/proj/output1/_SUCCESS\n",
    "#hdfs dfs -cat /user/hadoop/proj/output1/part-00000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute document frequency and add it to the previous output\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D stream.non.zero.exit.is.failure=false \\\n",
    "-input /user/hadoop/proj/output1 \\\n",
    "-output /user/hadoop/proj/output2 \\\n",
    "-file /home/hadoop/dmapper.py \\\n",
    "-mapper /home/hadoop/dmapper.py \\\n",
    "-file /home/hadoop/dreducer.py \\\n",
    "-reducer /home/hadoop/dreducer.py\n",
    "\n",
    "#hdfs dfs -ls /user/hadoop/proj/output2/\n",
    "hdfs dfs -rm /user/hadoop/proj/output2/_SUCCESS\n",
    "#hdfs dfs -cat /user/hadoop/proj/output2/part-00000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute tf-idf score\n",
    "\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D stream.non.zero.exit.is.failure=false \\\n",
    "-input /user/hadoop/proj/output2 \\\n",
    "-output /user/hadoop/proj/output3 \\\n",
    "-file /home/hadoop/tfidfmapper.py \\\n",
    "-mapper /home/hadoop/tfidfmapper.py\n",
    "\n",
    "\n",
    "#hdfs dfs -ls /user/hadoop/proj/output3/\n",
    "#hdfs dfs -cat /user/hadoop/proj/output3/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tmapper.py\n",
    "\n",
    "#!/usr/bin/python\n",
    " \n",
    "import sys\n",
    "import os\n",
    " \n",
    "for line in sys.stdin:\n",
    "    # get the file path\n",
    "    input_fn = os.getenv('mapreduce_map_input_file')\n",
    "    # only keep the name of file\n",
    "    name = str(input_fn).split(\"/\")[-1]\n",
    "    word = str(line.strip())\n",
    "    word_doc = word + \",\" + name\n",
    "    print '%s\\t%s' % (word_doc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# treducer.py\n",
    "\n",
    "\n",
    "#!/usr/bin/python\n",
    " \n",
    "import sys\n",
    " \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    " \n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    " \n",
    "    # parse the input we got from mapper.py\n",
    "    line = line.split('\\t')\n",
    "    word = line[0]\n",
    "    count = line[1]\n",
    " \n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    " \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "    \n",
    "# output ((word,file),frequency of the word in the file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfmapper.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "# use result of treducer.py, and output (filename, (word,frequency of the word in the file))\n",
    "import sys\n",
    " \n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    kv = line.split('\\t')\n",
    "    wordfilename = kv[0]\n",
    "    count = kv[1]\n",
    "    wf = wordfilename.split(',')\n",
    "    word = wf[0]\n",
    "    filename = wf[1]\n",
    "    wc = word + ','+count\n",
    "    print '%s\\t%s' % (filename, wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfreducer.py\n",
    "\n",
    "#!/usr/bin/python\n",
    " \n",
    "import sys\n",
    " \n",
    "current_word = None\n",
    "prev_filename = None\n",
    "current_count = 0\n",
    "word = None\n",
    "N=0\n",
    "df={} # this dictionary will store (key = name of file, value = size of that file)\n",
    "result=[] # this list will store the output of tmapper.py\n",
    " \n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    result.append(line)\n",
    "    filename = line.split(\"\\t\")[0]\n",
    "    wordcount = line.split(\"\\t\")[1]\n",
    "    word = wordcount.split(',')[0]\n",
    "    count = wordcount.split(',')[1]\n",
    "    count=int(count)\n",
    "    if prev_filename == filename:\n",
    "        N=N+count\n",
    "    else:\n",
    "        if prev_filename != None:\n",
    "            df[prev_filename]=N\n",
    "        N=count\n",
    "        prev_filename = filename\n",
    "df[prev_filename]=N\n",
    " \n",
    "for ele in result:\n",
    "    filename = ele.split('\\t')[0]\n",
    "    wordcount = ele.split('\\t')[1]\n",
    "    word = wordcount.split(',') [0]\n",
    "    count = wordcount.split(',') [1]\n",
    "    for name in df:\n",
    "        if filename == name:\n",
    "            wf=word+','+filename\n",
    "            nN=str(count)+','+str(df[name])            \n",
    "            print '%s\\t%s' % (wf,nN)\n",
    "            \n",
    "# output ((word,filename), (frequency of the word, size of that document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dmapper.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "# use the output of tfreducer.py\n",
    "import sys\n",
    " \n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    wf = line.split('\\t')[0]\n",
    "    nN = line.split('\\t')[1]\n",
    "    w = wf.split(',')[0]\n",
    "    f = wf.split(',')[1]\n",
    "    fn=f+','+nN+','+str(1)\n",
    "    print '%s\\t%s' % (w,fn)\n",
    "    \n",
    "# output (word,(filename,frequency of the word, size of that document,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dreducer.py\n",
    "\n",
    "#!/usr/bin/python\n",
    " \n",
    "import sys\n",
    " \n",
    "prev_word = None\n",
    "count = 0 # this will be variable for document frequency\n",
    " \n",
    "df={} # will store (key = word, value = (filename,frequency of the word, size of that document,document frequency))\n",
    "result=[] # will store (word,filename)\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    w = line.split('\\t')[0]\n",
    "    fn = line.split('\\t')[1]\n",
    "    f = fn.split(',')[0]\n",
    "    n = fn.split(',')[1]\n",
    "    N = fn.split(',')[2]\n",
    "    c = fn.split(',')[3]\n",
    "    if prev_word == w:\n",
    "        count = count+int(c)\n",
    "    else:\n",
    "        if prev_word != None:\n",
    "            q=n+','+N+','+str(count)\n",
    "            df[prev_word]=q\n",
    "            j=prev_word+','+f\n",
    "            result.append(j)\n",
    "        count=1\n",
    "        prev_word = w\n",
    " \n",
    "        \n",
    "q=n+','+N+','+str(count)\n",
    "df[prev_word]=q\n",
    "j=prev_word+','+f\n",
    "result.append(j)\n",
    " \n",
    "for ele in result:\n",
    "    w = ele.split(',')[0]\n",
    "    f = ele.split(',')[1]\n",
    "    for d in df:\n",
    "        if w == d:\n",
    "            print '%s\\t%s' % (ele,df[d])\n",
    "# output ((word,filename),(frequency of the word, size of that document,document frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tfidfmapper.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "# use output of dreducer.py, calculate the score, and output ((word,filename),tf-idf score)\n",
    " \n",
    "import sys\n",
    "import math\n",
    " \n",
    "D = 5.0\n",
    " \n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    wf,nNm=line.split('\\t',1)\n",
    "    wf = line.split('\\t')[0]\n",
    "    nNd = line.split('\\t')[1]\n",
    "    n = nNd.split(',')[0]\n",
    "    N = nNd.split(',')[1]\n",
    "    d = nNd.split(',')[2]\n",
    "    n=float(n)\n",
    "    N=float(N)\n",
    "    d=float(d)\n",
    "    tfidf= (n/N)*math.log(D/d)\n",
    "    print '%s\\t%s' % (wf,tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
